# Configuration for Human-Like English Rewriting System

# Model Configuration
model:
  base_model: "t5-base"  # Options: t5-base, t5-small, facebook/bart-base, google/pegasus-xsum
  use_pretrained_paraphrase: true  # Use pretrained paraphrase model if available
  pretrained_paraphrase_model: "humarin/chatgpt_paraphrase"  # Alternative: "tuner007/pegasus_paraphrase"
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1  # Restored to original - was working well before
  target_modules: ["q", "v"]  # For T5

# Training Configuration
training:
  learning_rate: 2e-4  # Restored to original - was working well before
  batch_size: 8
  gradient_accumulation_steps: 4
  num_epochs: 3  # Restored to original - was working well before
  max_length: 512
  warmup_steps: 500
  save_steps: 1000
  eval_steps: 500
  logging_steps: 100
  output_dir: "./models/checkpoints"
  save_total_limit: 3

# Data Configuration
data:
  datasets:
    # Academic/Research focused datasets
    # NOTE: Disabled - dataset only has paraphrased texts without aligned originals,
    # which produced corrupted pairs and caused the model to copy inputs.
    - name: "jpwahle/machine-paraphrase-dataset"
      split: "train"
      max_samples: 0  # Leave at 0 unless you supply aligned originals
    - name: "paws"
      config: "labeled_final"
      split: "train"
      max_samples: 30000  # High quality paraphrase pairs
    - name: "paws"
      config: "labeled_swap"
      split: "train"
      max_samples: 20000  # Additional PAWS config
    # General paraphrase datasets
    - name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
      split: "train"
      max_samples: 0  # Skip - this is a model
  
  preprocessing:
    min_length: 10
    max_length: 512  # Will be truncated during tokenization, but we allow longer texts here
    min_similarity: 0.0  # Disabled for speed (set > 0 to enable semantic similarity check)
    max_char_similarity: 0.95  # Maximum character-level similarity (to filter duplicates)
    filter_language: true  # Filter to keep only target language (default: English)
    target_language: "en"  # Target language code (ISO 639-1)
  
  splits:
    train: 0.8
    validation: 0.1
    test: 0.1

# Style Control Tokens
style:
  tones:
    - "formal"
    - "academic"
    - "casual"
  
  strengths:
    - "light"
    - "medium"
    - "strong"

# Evaluation Configuration
evaluation:
  metrics:
    - "bertscore"
    - "rouge"
    - "fluency"
    - "diversity"
  
  bertscore_model: "bert-base-uncased"
  sample_size: 1000

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  max_length: 512
  timeout: 30
  workers: 1

# Paths
paths:
  data_dir: "./data"
  raw_data_dir: "./data/raw"
  processed_data_dir: "./data/processed"
  models_dir: "./models"
  checkpoints_dir: "./models/checkpoints"
  final_model_dir: "./models/final"
