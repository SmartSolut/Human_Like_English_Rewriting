"""
FastAPI Application
REST API for Human-Like English Rewriting System
"""

import os
import yaml
import torch
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import PeftModel
from typing import Optional
import time


# Load config
with open("config.yaml", 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

# Initialize FastAPI app
app = FastAPI(
    title="Human-Like English Rewriting API",
    description="API for rewriting English text in a human-like manner",
    version="1.0.0"
)

# Add CORS middleware to allow requests from HTML files
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins (you can restrict this in production)
    allow_credentials=True,
    allow_methods=["*"],  # Allow all methods
    allow_headers=["*"],  # Allow all headers
)

# Global model variables
model = None
tokenizer = None
device = None


class RewriteRequest(BaseModel):
    text: str = Field(..., description="Input text to rewrite", min_length=1, max_length=2000)
    tone: Optional[str] = Field("casual", description="Writing tone: formal, academic, or casual")
    strength: Optional[str] = Field("medium", description="Rewriting strength: light, medium, or strong")
    max_length: Optional[int] = Field(None, description="Maximum output length")


class RewriteResponse(BaseModel):
    original: str
    rewritten: str
    tone: str
    strength: str
    processing_time: float


def _find_latest_checkpoint(checkpoints_dir):
    """Find the latest checkpoint in checkpoints directory"""
    if not os.path.exists(checkpoints_dir):
        return None
    
    # Find all checkpoint directories
    checkpoints = [
        d for d in os.listdir(checkpoints_dir)
        if os.path.isdir(os.path.join(checkpoints_dir, d)) and d.startswith("checkpoint-")
    ]
    
    if not checkpoints:
        return None
    
    # Sort by checkpoint number (extract number from "checkpoint-XXXX")
    def get_checkpoint_number(checkpoint_name):
        try:
            return int(checkpoint_name.split("-")[1])
        except (ValueError, IndexError):
            return 0
    
    checkpoints.sort(key=get_checkpoint_number)
    latest_checkpoint = os.path.join(checkpoints_dir, checkpoints[-1])
    
    # Verify checkpoint has required files
    has_adapter = os.path.exists(os.path.join(latest_checkpoint, "adapter_config.json"))
    has_tokenizer = any([
        os.path.exists(os.path.join(latest_checkpoint, "tokenizer.json")),
        os.path.exists(os.path.join(latest_checkpoint, "tokenizer_config.json"))
    ])
    
    if has_adapter or has_tokenizer:
        return latest_checkpoint
    
    return None


def load_model():
    """Load the trained model or fallback to base model"""
    global model, tokenizer, device
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Loading model on {device}...")
    
    model_path = config['paths']['final_model_dir']
    checkpoints_dir = config['paths']['checkpoints_dir']
    base_model_name = config['model']['base_model']
    style_config = config.get('style', {})
    tone_tokens = [f"<tone={t}>" for t in style_config.get('tones', [])]
    strength_tokens = [f"<strength={s}>" for s in style_config.get('strengths', [])]
    special_tokens = list({*tone_tokens, *strength_tokens})
    
    # Try to load trained model, fallback to base model
    # First check final model directory
    has_adapter = os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "adapter_config.json"))
    has_full_model = os.path.exists(model_path) and os.path.exists(os.path.join(model_path, "config.json"))
    
    # If final model not found, try latest checkpoint
    if not (has_adapter or has_full_model):
        print(f"‚ö†Ô∏è  Final model not found at {model_path}, checking for latest checkpoint...")
        latest_checkpoint = _find_latest_checkpoint(checkpoints_dir)
        if latest_checkpoint:
            checkpoint_adapter = os.path.join(latest_checkpoint, "adapter_config.json")
            checkpoint_config = os.path.join(latest_checkpoint, "config.json")
            if os.path.exists(checkpoint_adapter) or os.path.exists(checkpoint_config):
                model_path = latest_checkpoint
                has_adapter = os.path.exists(checkpoint_adapter)
                has_full_model = os.path.exists(checkpoint_config)
                print(f"‚úÖ Using latest checkpoint: {model_path}")
    
    if has_adapter or has_full_model:
        try:
            print(f"Loading trained model from {model_path}...")
            # Load tokenizer first
            tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # Load base model
            base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)
            
            # Resize embeddings to match tokenizer vocab size (IMPORTANT!)
            base_model.resize_token_embeddings(len(tokenizer))
            
            # Check if LoRA model
            if has_adapter:
                model = PeftModel.from_pretrained(base_model, model_path)
                print("‚úÖ Loaded fine-tuned LoRA model successfully!")
            else:
                model = base_model
                print("‚úÖ Loaded fine-tuned base model successfully!")
        except Exception as e:
            print(f"‚ùå Error loading trained model: {e}")
            import traceback
            traceback.print_exc()
            print("Falling back to base model...")
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)
            print(f"‚ö†Ô∏è  Loaded base model: {base_model_name} (not fine-tuned)")
    else:
        # Use pretrained paraphrase model if configured, otherwise base model
        use_pretrained = config['model'].get('use_pretrained_paraphrase', False)
        pretrained_model_name = config['model'].get('pretrained_paraphrase_model', None)
        
        if use_pretrained and pretrained_model_name:
            try:
                print(f"Trained model not found. Trying pretrained paraphrase model: {pretrained_model_name}")
                tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)
                model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name)
                print(f"Loaded pretrained paraphrase model: {pretrained_model_name}")
            except Exception as e:
                print(f"Error loading pretrained model: {e}")
                print("Falling back to base T5 model...")
                tokenizer = AutoTokenizer.from_pretrained(base_model_name)
                model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)
                print("Loaded base T5 model (not fine-tuned - results may be poor)")
        else:
            print(f"Trained model not found at {model_path}")
            print("Using base model for testing (not fine-tuned - results may be poor)...")
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)
            print(f"Loaded base model: {base_model_name} (not fine-tuned)")
    
    # Don't add tokens again if model was loaded from saved path (they're already in tokenizer)
    # Only add if using base/pretrained model
    if not (has_adapter or has_full_model):
        if special_tokens:
            tokenizer.add_tokens(special_tokens, special_tokens=False)
            model.resize_token_embeddings(len(tokenizer))
    
    model.to(device)
    model.eval()
    
    # Log detailed model information
    print(f"\n{'='*60}")
    print("Model Loaded Successfully!")
    print(f"{'='*60}")
    print(f"üìÅ Model Path: {model_path}")
    print(f"üì¶ Base Model: {base_model_name}")
    print(f"üîß Model Type: {'LoRA Adapter' if has_adapter else 'Full Model' if has_full_model else 'Base Model'}")
    print(f"üíª Device: {device}")
    
    # Check model size
    if os.path.exists(model_path):
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(model_path):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if os.path.exists(fp):
                    total_size += os.path.getsize(fp)
        size_mb = total_size / (1024 * 1024)
        print(f"üì¶ Model Size: {size_mb:.2f} MB")
        
        if has_adapter and size_mb > 50:
            print(f"‚ö†Ô∏è  Warning: LoRA adapter size is unusually large!")
            print(f"   Expected: ~5-15 MB, Got: {size_mb:.2f} MB")
    
    print(f"{'='*60}\n")


@app.on_event("startup")
async def startup_event():
    """Load model on startup"""
    try:
        load_model()
    except Exception as e:
        print(f"Error loading model: {e}")
        print("API will start but rewrite endpoint may not work properly.")


@app.get("/", response_class=HTMLResponse)
async def root():
    """Simple web interface"""
    html_content = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Human-Like English Rewriting</title>
        <style>
            * {
                margin: 0;
                padding: 0;
                box-sizing: border-box;
            }
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
                padding: 20px;
                line-height: 1.6;
            }
            .container {
                max-width: 1400px;
                margin: 0 auto;
                background: white;
                border-radius: 20px;
                box-shadow: 0 20px 60px rgba(0,0,0,0.3);
                overflow: hidden;
            }
            .header {
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                padding: 40px 30px;
                text-align: center;
            }
            .header h1 {
                font-size: 2.5em;
                margin-bottom: 10px;
                font-weight: 700;
            }
            .header .subtitle {
                font-size: 1.2em;
                opacity: 0.95;
            }
            .controls-section {
                padding: 30px;
                background: #f8f9fa;
                border-bottom: 1px solid #e0e0e0;
            }
            .options-row {
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 20px;
                margin-bottom: 20px;
            }
            .form-group {
                margin-bottom: 0;
            }
            label {
                display: block;
                margin-bottom: 8px;
                color: #333;
                font-weight: 600;
                font-size: 1em;
            }
            select {
                width: 100%;
                padding: 12px;
                border: 2px solid #e0e0e0;
                border-radius: 10px;
                font-size: 1em;
                font-family: inherit;
                background: white;
                cursor: pointer;
                transition: border-color 0.3s;
            }
            select:focus {
                outline: none;
                border-color: #667eea;
            }
            .main-content {
                display: grid;
                grid-template-columns: 1fr 1fr;
                gap: 30px;
                padding: 30px;
            }
            .text-box-container {
                display: flex;
                flex-direction: column;
                height: 100%;
            }
            .text-box-label {
                font-weight: 600;
                margin-bottom: 10px;
                color: #333;
                font-size: 1.1em;
            }
            .text-box {
                flex: 1;
                min-height: 400px;
                padding: 20px;
                border: 2px solid #e0e0e0;
                border-radius: 10px;
                font-size: 1em;
                font-family: inherit;
                resize: vertical;
                transition: border-color 0.3s;
            }
            .text-box:focus {
                outline: none;
                border-color: #667eea;
            }
            .text-box.output {
                background: #f8f9fa;
                cursor: default;
            }
            .word-count {
                margin-top: 10px;
                color: #666;
                font-size: 0.9em;
            }
            .action-buttons {
                display: flex;
                gap: 15px;
                justify-content: center;
                margin: 30px 0;
            }
            .btn-primary {
                padding: 15px 40px;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                color: white;
                border: none;
                border-radius: 30px;
                font-size: 1.1em;
                font-weight: 600;
                cursor: pointer;
                transition: all 0.3s ease;
                box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
            }
            .btn-primary:hover:not(:disabled) {
                transform: translateY(-2px);
                box-shadow: 0 8px 20px rgba(102, 126, 234, 0.5);
            }
            .btn-primary:active:not(:disabled) {
                transform: translateY(0);
            }
            .btn-primary:disabled {
                opacity: 0.6;
                cursor: not-allowed;
            }
            .btn-secondary {
                padding: 15px 40px;
                background: white;
                color: #667eea;
                border: 2px solid #667eea;
                border-radius: 30px;
                font-size: 1.1em;
                font-weight: 600;
                cursor: pointer;
                transition: all 0.3s ease;
            }
            .btn-secondary:hover {
                background: #667eea;
                color: white;
                transform: translateY(-2px);
            }
            .loading {
                text-align: center;
                color: #667eea;
                margin: 20px 0;
                font-size: 1.1em;
            }
            .error {
                background: #fee;
                border: 1px solid #f44;
                border-radius: 10px;
                padding: 15px;
                margin: 20px 30px;
                color: #c33;
                display: none;
            }
            .error.show {
                display: block;
            }
            .stats {
                text-align: center;
                color: #666;
                margin-top: 15px;
                font-size: 0.9em;
            }
            .copy-btn {
                margin-top: 10px;
                padding: 8px 20px;
                background: #28a745;
                color: white;
                border: none;
                border-radius: 20px;
                cursor: pointer;
                font-size: 0.9em;
                transition: all 0.3s;
            }
            .copy-btn:hover {
                background: #218838;
                transform: translateY(-1px);
            }
            @media (max-width: 768px) {
                .header h1 {
                    font-size: 1.8em;
                }
                .main-content {
                    grid-template-columns: 1fr;
                    gap: 20px;
                    padding: 20px;
                }
                .text-box {
                    min-height: 300px;
                }
                .options-row {
                    grid-template-columns: 1fr;
                }
                .action-buttons {
                    flex-direction: column;
                }
                .btn-primary, .btn-secondary {
                    width: 100%;
                }
            }
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üîÑ Human-Like English Rewriting</h1>
                <p class="subtitle">Rewrite your text naturally while preserving meaning</p>
            </div>
            
            <div class="controls-section">
                <div class="options-row">
                    <div class="form-group">
                        <label for="tone">Tone:</label>
                        <select id="tone">
                            <option value="casual">Casual</option>
                            <option value="formal">Formal</option>
                            <option value="academic">Academic</option>
                        </select>
                    </div>
                    <div class="form-group">
                        <label for="strength">Strength (ŸÇŸàÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑÿµŸäÿßÿ∫ÿ©):</label>
                        <select id="strength">
                            <option value="light">ŸÖŸÜÿÆŸÅÿ∂ (Light)</option>
                            <option value="medium" selected>ŸÖÿ™Ÿàÿ≥ÿ∑ (Medium)</option>
                            <option value="strong">ŸÖÿ±ÿ™ŸÅÿπ (Strong)</option>
                            <option value="ultra run">ÿπÿßŸÑŸä (Ultra)</option>
                        </select>
                    </div>
                </div>
            </div>
            
            <div class="error" id="error"></div>
            
            <div class="main-content">
                <div class="text-box-container">
                    <label class="text-box-label">Insert (English) text here</label>
                    <textarea id="inputText" class="text-box" placeholder="Type or paste your text here..."></textarea>
                    <div class="word-count" id="inputWordCount">0 words</div>
                </div>
                <div class="text-box-container">
                    <label class="text-box-label">Paraphrased text will appear here</label>
                    <div id="outputText" class="text-box output" contenteditable="false"></div>
                    <div class="word-count" id="outputWordCount">0 words</div>
                    <button class="copy-btn" id="copyBtn" style="display: none;">Copy Text</button>
                </div>
            </div>
            
            <div class="action-buttons">
                <button class="btn-primary" id="submitBtn">Rewrite Text</button>
                <button class="btn-secondary" id="clearBtn">Clear</button>
            </div>
            
            <div class="loading" id="loading" style="display: none;">
                Processing your text... Please wait
            </div>
            
            <div class="stats" id="stats"></div>
        </div>
        
        <script>
            const inputText = document.getElementById('inputText');
            const outputText = document.getElementById('outputText');
            const inputWordCount = document.getElementById('inputWordCount');
            const outputWordCount = document.getElementById('outputWordCount');
            const submitBtn = document.getElementById('submitBtn');
            const clearBtn = document.getElementById('clearBtn');
            const copyBtn = document.getElementById('copyBtn');
            const loading = document.getElementById('loading');
            const error = document.getElementById('error');
            const stats = document.getElementById('stats');
            
            function countWords(text) {
                return text.trim().split(/\\s+/).filter(word => word.length > 0).length;
            }
            
            function updateWordCount() {
                const inputWords = countWords(inputText.value);
                const outputWords = countWords(outputText.textContent);
                inputWordCount.textContent = `${inputWords} word${inputWords !== 1 ? 's' : ''}`;
                outputWordCount.textContent = `${outputWords} word${outputWords !== 1 ? 's' : ''}`;
            }
            
            inputText.addEventListener('input', updateWordCount);
            
            submitBtn.addEventListener('click', async function() {
                const text = inputText.value.trim();
                const tone = document.getElementById('tone').value;
                const strength = document.getElementById('strength').value;
                
                if (!text) {
                    error.textContent = 'Please enter some text to rewrite.';
                    error.classList.add('show');
                    return;
                }
                
                error.classList.remove('show');
                loading.style.display = 'block';
                submitBtn.disabled = true;
                outputText.textContent = '';
                stats.textContent = '';
                copyBtn.style.display = 'none';
                
                try {
                    const response = await fetch('/api/rewrite', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({
                            text: text,
                            tone: tone,
                            strength: strength
                        })
                    });
                    
                    const data = await response.json();
                    
                    if (!response.ok) {
                        throw new Error(data.detail || 'An error occurred');
                    }
                    
                    outputText.textContent = data.rewritten;
                    stats.textContent = `Processing time: ${data.processing_time.toFixed(2)}s | Tone: ${data.tone} | Strength: ${data.strength}`;
                    copyBtn.style.display = 'block';
                    updateWordCount();
                    
                } catch (err) {
                    error.textContent = 'Error: ' + err.message;
                    error.classList.add('show');
                } finally {
                    loading.style.display = 'none';
                    submitBtn.disabled = false;
                }
            });
            
            clearBtn.addEventListener('click', function() {
                inputText.value = '';
                outputText.textContent = '';
                updateWordCount();
                error.classList.remove('show');
                stats.textContent = '';
                copyBtn.style.display = 'none';
            });
            
            copyBtn.addEventListener('click', function() {
                const text = outputText.textContent;
                navigator.clipboard.writeText(text).then(() => {
                    const originalText = this.textContent;
                    this.textContent = 'Copied!';
                    this.style.background = '#218838';
                    setTimeout(() => {
                        this.textContent = originalText;
                        this.style.background = '#28a745';
                    }, 2000);
                }).catch(err => {
                    alert('Failed to copy text. Please select and copy manually.');
                });
            });
            
            updateWordCount();
        </script>
    </body>
    </html>
    """
    return html_content


@app.post("/api/rewrite", response_model=RewriteResponse)
async def rewrite_text(request: RewriteRequest):
    """Rewrite text endpoint"""
    if model is None or tokenizer is None:
        # Try to load model if not loaded
        try:
            load_model()
        except Exception as e:
            raise HTTPException(
                status_code=503,
                detail=f"Model not loaded: {str(e)}"
            )
    
    start_time = time.time()
    
    try:
        # Prepare input with prompt format used during training
        tone = request.tone or "casual"
        strength = request.strength or "medium"
        
        # Handle "ultra run" - treat as "strong" for maximum variation
        if strength and ("ultra" in strength.lower() or strength.lower() == "ultra run"):
            strength = "strong"
        
        # Always use simple prompt format (model was trained on this)
        input_text = f"humanize: {request.text}"
        
        # Determine max length
        max_length = request.max_length or config['training']['max_length']
        
        # Tokenize with dynamic padding (more efficient)
        inputs = tokenizer(
            input_text,
            max_length=max_length,
            padding=True,  # Dynamic padding
            truncation=True,
            return_tensors='pt'
        ).to(device)
        
        # Calculate reasonable output length
        # Model was trained on max_target_length=256, so allow up to that
        input_length = inputs['input_ids'].shape[1]
        # Increase min_length to ensure output is at least 80% of input length
        min_output_length = max(20, int(input_length * 0.8))  # At least 80% of input (increased from 50%)
        # Allow longer outputs - up to 1.3x input or max training length, whichever is larger
        input_word_count = len(request.text.split())
        max_output_tokens = max(int(input_length * 1.3), int(input_word_count * 1.3 / 0.75))  # Account for token/word ratio
        max_output_length = min(max_length, max(256, max_output_tokens))  # At least 256 tokens, or more if needed
        
        # Adjust generation parameters based on tone and strength
        # Optimized for better paraphrasing with more variation
        
        # Base parameters - increased variation for better paraphrasing
        base_temperature = 1.0  # Higher temperature for more variation
        base_repetition_penalty = 1.8  # Lower penalty to allow more natural variation
        base_length_penalty = 1.0  # Neutral length penalty
        
        # Adjust based on strength - BALANCED TO PRESERVE GRAMMAR AND CONNECTORS
        if strength == "light":
            temperature = 1.15  # Slightly reduced to preserve small words
            repetition_penalty = 1.5  # Higher penalty to preserve connectors
            length_penalty = 1.05  # Slightly favor longer outputs
            num_beams = 2  # Balanced beams for quality
            do_sample = True  # Enable sampling for variation
        elif strength == "strong" or strength == "ultra" or "ultra" in str(strength).lower():  # Handle "ultra run"
            temperature = 1.5  # Reduced to preserve grammar and connectors
            repetition_penalty = 1.65  # Higher penalty to preserve small words
            length_penalty = 1.3  # Favor longer outputs
            num_beams = 2  # Balanced beams for quality and variation
            do_sample = True  # Enable sampling for variation
        else:  # medium - DEFAULT - should have good variation
            temperature = 1.35  # Reduced to preserve grammar
            repetition_penalty = 1.55  # Higher penalty to preserve connectors
            length_penalty = 1.15  # Favor longer outputs
            num_beams = 2  # Balanced beams for quality
            do_sample = True  # Enable sampling for variation
        
        # Adjust based on tone (affects length and formality)
        if tone == "academic":
            temperature = max(0.5, temperature - 0.12)  # More conservative for academic to preserve grammar
            repetition_penalty = max(1.6, repetition_penalty + 0.08)  # Higher penalty to preserve connectors
            length_penalty += 0.2  # More favor for longer outputs in academic writing
            min_output_length = max(min_output_length, int(input_length * 0.9))  # Academic needs more length
        elif tone == "formal":
            temperature = max(0.5, temperature - 0.08)  # More conservative for formal
            repetition_penalty = max(1.55, repetition_penalty + 0.05)  # Higher penalty to preserve connectors
            length_penalty += 0.15  # Favor longer outputs for formal tone
            min_output_length = max(min_output_length, int(input_length * 0.85))  # Formal needs adequate length
        
        # Calculate reasonable max_length based on input length
        # Allow longer outputs but prevent excessive over-generation
        input_token_count = inputs['input_ids'].shape[1]
        # Use at least 1.3x input, but allow up to max_output_length for longer texts
        reasonable_max = max(int(input_token_count * 1.3), min(max_output_length, int(input_token_count * 2.0)))  # Min 1.3x, max 2.0x or max_output_length
        
        # Prepare generation kwargs
        # Note: early_stopping with num_beams stops when ALL beams find EOS, but min_length ensures minimum length
        gen_kwargs = {
            "max_length": reasonable_max,  # Use reasonable max instead of fixed max
            "min_length": min_output_length,
            "num_beams": num_beams,
            "early_stopping": False,  # Don't stop early - generate full length to reach min_length
            "repetition_penalty": repetition_penalty,
            "length_penalty": length_penalty,
            "no_repeat_ngram_size": 3,  # Higher to preserve grammatical structures and connectors
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
        }
        
        # Add sampling parameters only if do_sample is True
        if do_sample:
            gen_kwargs.update({
                "do_sample": True,
                "temperature": temperature,
                "top_p": 0.93,  # Slightly reduced to preserve grammatical words
                "top_k": 42,  # Slightly reduced to preserve connectors
            })
        
        # Generate with tone/strength-adjusted parameters
        with torch.no_grad():
            outputs = model.generate(
                **inputs,  # Unpack inputs dict (input_ids, attention_mask)
                **gen_kwargs
            )
        
        # Decode and clean
        # Use clean_up_tokenization_spaces to fix encoding issues
        rewritten = tokenizer.decode(
            outputs[0], 
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True  # Fix spacing and encoding issues
        )
        
        # Clean up the output - remove control tokens and prefix if present
        import re
        # Remove control tokens
        rewritten = re.sub(r'<tone=[^>]+>', '', rewritten)
        rewritten = re.sub(r'<strength=[^>]+>', '', rewritten)
        # Remove prompt prefixes
        if rewritten.startswith("humanize:"):
            rewritten = rewritten.replace("humanize:", "").strip()
        elif rewritten.startswith("paraphrase:"):
            rewritten = rewritten.replace("paraphrase:", "").strip()
        
        # ===== PRESERVE CRITICAL ELEMENTS: Numbers, Formulas, Geographic Names =====
        original_text = request.text
        
        # Fix common corruptions in rewritten text
        # 1. Fix geographic name corruptions
        rewritten = re.sub(r'\bSaigon\b', 'Riyadh', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bFirst Wellness Cluster\b', 'First Health Cluster', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bWellness Cluster\b', 'Health Cluster', rewritten, flags=re.IGNORECASE)
        
        # 2. Fix number corruptions (commas to dots, superscripts, spaces in decimals)
        # Fix spaces in decimal numbers (e.g., "64. 8%" -> "64.8%")
        rewritten = re.sub(r'(\d+)\.\s+(\d+)', r'\1.\2', rewritten)  # "64. 8%" -> "64.8%"
        rewritten = re.sub(r'(\d+)\s*,\s*(\d+)\s*2\b', r'\1.\2¬≤', rewritten)  # "1, 962" -> "1.96¬≤"
        rewritten = re.sub(r'(\d+)\s*,\s*(\d+)\b', r'\1.\2', rewritten)  # "0, 5" -> "0.5"
        rewritten = re.sub(r'(\d+\.\d+)\s*2\b', r'\1¬≤', rewritten)  # "1.96 2" -> "1.96¬≤"
        rewritten = re.sub(r'Z\s*2\b', 'Z¬≤', rewritten, flags=re.IGNORECASE)  # "Z2" -> "Z¬≤"
        rewritten = re.sub(r'd\s*2\b', 'd¬≤', rewritten, flags=re.IGNORECASE)  # "d2" -> "d¬≤"
        rewritten = re.sub(r'(\d+)\s*¬≤\s*', r'\1¬≤', rewritten)  # "1.96 ¬≤" -> "1.96¬≤"
        
        # Fix corrupted mathematical symbols (e.g., "had¬≤ years" -> "had ‚â§2 years")
        rewritten = re.sub(r'\bhad¬≤\s+years\b', 'had ‚â§2 years', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bhad\s*¬≤\s*(\d+)\s*years\b', r'had ‚â§\1 years', rewritten, flags=re.IGNORECASE)
        # Fix other common symbol corruptions
        rewritten = re.sub(r'‚â§\s*2', '‚â§2', rewritten)  # "‚â§ 2" -> "‚â§2"
        rewritten = re.sub(r'‚â•\s*(\d+)', r'‚â•\1', rewritten)  # "‚â• 5" -> "‚â•5"
        
        # 3. Fix formula corruptions
        # Restore proper formula format: "n = (N √ó Z¬≤ √ó P (1 - P)) / (d¬≤ √ó (N - 1) + Z¬≤ √ó P (1 - P))"
        rewritten = re.sub(r'n\s*=\s*\(N\s+Z2\s+P\s*\(1\s*-\s*P\)\)', 'n = (N √ó Z¬≤ √ó P (1 - P))', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'/\s*\(d2\s*\*\s*1\s*\(N\s*1\)', ' / (d¬≤ √ó (N - 1)', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'Z2\s*3\s*N\s*\(1\s*‚Äì\s*P\)', 'Z¬≤ √ó P (1 - P)', rewritten, flags=re.IGNORECASE)
        
        # 4. Fix variable corruptions in formulas
        rewritten = re.sub(r'\bN\s*=\s*223\s*\(total\s+population\s+associated\s+with\s+a\s+95%\s+confidence\s+level\)', 
                          'N = 223 (total population of eligible family physicians)', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'P\s*=\s*0\.\s*5\s*\(expected\s+proportion', 
                          'P = 0.5 (expected proportion', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'd\s*=\s*0,\s*5\s*\(margin\s+of\s+error\)', 
                          'd = 0.05 (margin of error)', rewritten, flags=re.IGNORECASE)
        
        # 5. Fix calculation corruptions
        rewritten = re.sub(r's\s*=\s*\(223\s+1,\s*962', 'n = (223 √ó 1.96¬≤', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'0\.\s*052\s*0\s*\(223√¢\s*√¢\s*1\)', '0.05¬≤ √ó (223 - 1)', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'1\.\s*962\s*%\s*0\.\s*1\s*1\.\s*5\s*\(1\)\s*-\s*0\.\s*5\)', 
                          '1.96¬≤ √ó 0.5 √ó (1 - 0.5)', rewritten, flags=re.IGNORECASE)
        
        # 6. Remove mojibake and encoding errors
        rewritten = re.sub(r'√¢\s*', ' ', rewritten)  # Remove mojibake "√¢"
        rewritten = re.sub(r'[^\x20-\x7E\u00A0-\uFFFF‚Ä¢¬≤¬≥‚Å¥‚Åµ√ó√∑]', '', rewritten)  # Keep valid Unicode and math symbols
        
        # 7. Preserve bullet points
        rewritten = re.sub(r'[‚Ä¢¬∑]\s*', '‚Ä¢ ', rewritten)  # Normalize bullet points
        
        # 8. Extract and restore exact numbers from original text if they're corrupted
        # Match all numbers from original (with context to find them in rewritten)
        original_numbers = re.findall(r'\b(\d+(?:\.\d+)?(?:¬≤|¬≥)?)\b', original_text)
        for orig_num in set(original_numbers):
            # Find corrupted versions in rewritten
            corrupted_versions = [
                orig_num.replace('.', ', '),
                orig_num.replace('¬≤', '2'),
                orig_num.replace('.', ', ').replace('¬≤', '2'),
            ]
            for corrupted in corrupted_versions:
                if corrupted in rewritten and orig_num not in rewritten:
                    # Replace with context to avoid false matches
                    rewritten = re.sub(r'\b' + re.escape(corrupted) + r'\b', orig_num, rewritten)
        
        # Fix encoding issues - remove mojibake characters
        # These appear when tokenizer produces invalid token sequences
        rewritten = re.sub(r'√¢√¢√¢[^\s]*', '', rewritten)  # Remove mojibake patterns
        rewritten = re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]+', '', rewritten)  # Keep only valid Unicode
        
        # Clean up artifacts but preserve multilingual characters
        # Remove excessive dots and dashes
        rewritten = re.sub(r'\.{3,}', '.', rewritten)  # Multiple dots to single
        rewritten = re.sub(r'-{3,}', '-', rewritten)  # Multiple dashes to single
        # Remove control characters but keep Unicode letters/numbers/punctuation
        rewritten = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', rewritten)  # Control chars only
        rewritten = re.sub(r'\s+', ' ', rewritten)  # Multiple spaces to single
        rewritten = rewritten.strip()
        
        # ===== NEW: Remove duplicate sentences and repetitions =====
        # Split into sentences
        sentences = re.split(r'[.!?]+', rewritten)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # Remove duplicate sentences (case-insensitive, ignoring punctuation)
        seen_sentences = set()
        unique_sentences = []
        for sentence in sentences:
            # Normalize sentence (lowercase, remove punctuation)
            normalized = re.sub(r'[^\w\s]', '', sentence.lower())
            if normalized and normalized not in seen_sentences:
                seen_sentences.add(normalized)
                unique_sentences.append(sentence)
        
        # Rejoin sentences
        if unique_sentences:
            rewritten = '. '.join(unique_sentences) + '.'
            rewritten = re.sub(r'\.{2,}', '.', rewritten)  # Remove multiple dots
        
        # Remove repetitive phrases (e.g., "released in June 2017 and released in May 2017")
        # Pattern: same phrase repeated with "and" or comma
        rewritten = re.sub(r'\b(\w+(?:\s+\w+){2,5})\s+(?:and|,)\s+\1\b', r'\1', rewritten, flags=re.IGNORECASE)
        
        # Remove similar repetitive patterns with dates/numbers
        rewritten = re.sub(r'\b(released|published|created|developed|distributed)\s+(?:in|on)\s+(\w+\s+\d{4})\s+and\s+\1\s+(?:in|on)\s+\w+\s+\d{4}\b', r'\1 in \2', rewritten, flags=re.IGNORECASE)
        
        # Check output length: if much shorter than input, don't truncate
        # Only truncate if output is excessively longer (2x or more)
        original_words = len(request.text.split())
        rewritten_words = len(rewritten.split())
        
        # Only truncate if output is more than 2x longer (was 1.5x)
        if rewritten_words > original_words * 2.0:
            # Keep first sentences that add up to reasonable length (1.5x max)
            truncated_sentences = []
            word_count = 0
            target_words = int(original_words * 1.5)  # 50% longer max (increased from 30%)
            
            for sentence in unique_sentences:
                sentence_words = len(sentence.split())
                if word_count + sentence_words <= target_words:
                    truncated_sentences.append(sentence)
                    word_count += sentence_words
                else:
                    break
            
            if truncated_sentences:
                rewritten = '. '.join(truncated_sentences) + '.'
                rewritten = re.sub(r'\.{2,}', '.', rewritten)
        
        # Remove sentences at the end that look like hallucinated information
        # Check if last sentence contains information not in original text
        if unique_sentences and len(unique_sentences) > 1:
            # Common patterns of hallucinated information
            suspicious_patterns = [
                r'\b(EA Games|Electronic Arts|Ubisoft|Activision|Sony|Microsoft|Nintendo).*(?:developed|published|released)\b',
                r'\b(developed|published|released)\s+by\s+[A-Z][a-z]+\s+(?:Games|Inc|Corp|LLC)',
                r'\b(Japan|China|Korea|Europe|America).*(?:published|released|developed)',
                r'\b(?:The game|This game|It)\s+(?:was|is)\s+(?:developed|published|released|created)',
            ]
            
            # Check last 2 sentences for suspicious patterns
            last_sentences = unique_sentences[-2:] if len(unique_sentences) >= 2 else [unique_sentences[-1]]
            original_lower = request.text.lower()
            
            sentences_to_keep = []
            for i, sentence in enumerate(unique_sentences):
                # If this is one of the last sentences, check if it's suspicious
                if sentence in last_sentences:
                    sentence_lower = sentence.lower()
                    is_suspicious = False
                    
                    # Check against suspicious patterns
                    for pattern in suspicious_patterns:
                        if re.search(pattern, sentence, flags=re.IGNORECASE):
                            is_suspicious = True
                            break
                    
                    # Check if sentence contains information not in original (simple check)
                    # Extract key entities (company names, dates, etc.)
                    key_entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*(?:\s+(?:Games|Inc|Corp|LLC|America|Japan|China|Korea))?\b', sentence)
                    if key_entities:
                        # Check if any key entity is not mentioned in original
                        for entity in key_entities:
                            if entity.lower() not in original_lower:
                                is_suspicious = True
                                break
                    
                    # Only remove if suspicious AND it's the last sentence(s)
                    if is_suspicious and i >= len(unique_sentences) - 2:
                        continue  # Skip this suspicious sentence
                
                sentences_to_keep.append(sentence)
            
            if sentences_to_keep:
                rewritten = '. '.join(sentences_to_keep) + '.'
                rewritten = re.sub(r'\.{2,}', '.', rewritten)
        
        # Final cleanup
        rewritten = re.sub(r'\s+', ' ', rewritten).strip()
        
        # Advanced post-processing: Remove repetitive patterns and suspicious words
        # Known problematic words from training data
        BLACKLIST_WORDS = {
            'particular', 'characteristic', 'associated', 'plus', 'front', 'friends',
            'troublesome', 'troubleshooting', 'troublesomeness', 'troubleworthyness',
            'otherworldlinestrezziness', 'troubleshoving', 'quizziness', 'heckplus',
            'hellabrinism', 'homeplus', 'friendswear', 'lovedplus', 'friendslines',
            'builtpertaining', 'associateddication', 'associatedwith', 'togetherness',
            'tenduousness', 'senselessness', 'latterness', 'firmness', 'array',
            'fixtures', 'outfitwear', 'accessory', 'pertaining', 'framework',
            # Add more problematic patterns from Part 3
            'troublesomeness', 'troubleshooting', 'prime', 'community', 'together',
            'frontlines', 'frontline', 'built', 'togetherness', 'identifiable',
            'target', 'features', 'aside', 'individuals', 'attention', 'interest',
            'friendsrequincement', 'outfitwear', 'fixtures', 'structures', 'towards',
            'utilized', 'specifically', 'designed', 'developed', 'against', 'within',
            'hellabrinism', 'accessory', 'individual', 'identifiable', 'with',
            'especially', 'aside', 'attention', 'togetherness'
        }
        
        # Split into words
        words = rewritten.split()
        
        # Remove words that appear too frequently (likely model collapse)
        word_counts = {}
        for word in words:
            word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
            if len(word_lower) > 3:  # Only count meaningful words
                word_counts[word_lower] = word_counts.get(word_lower, 0) + 1
        
        # More aggressive threshold: 15% instead of 30%
        total_meaningful_words = sum(1 for w in words if len(w.lower().strip('.,!?;:()[]{}"\'-')) > 3)
        if total_meaningful_words > 0:
            threshold = max(2, int(total_meaningful_words * 0.15))  # 15% threshold, min 2
            suspicious_words = {w for w, count in word_counts.items() if count > threshold}
            
            # Also add blacklisted words that appear more than once
            for word_lower, count in word_counts.items():
                if word_lower in BLACKLIST_WORDS and count > 1:
                    suspicious_words.add(word_lower)
            
            # Remove suspicious words that appear too frequently
            if suspicious_words:
                filtered_words = []
                seen_suspicious = {}  # Track how many times we've seen each suspicious word
                for word in words:
                    word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
                    
                    # Always remove blacklisted words completely (don't allow even first occurrence)
                    if word_lower in BLACKLIST_WORDS:
                        continue  # Skip all blacklisted words
                    
                    # Handle other suspicious words
                    if word_lower not in suspicious_words or len(word_lower) <= 3:
                        filtered_words.append(word)
                    else:
                        # Keep only first occurrence of suspicious word
                        if word_lower not in seen_suspicious:
                            seen_suspicious[word_lower] = 0
                        seen_suspicious[word_lower] += 1
                        if seen_suspicious[word_lower] == 1:
                            filtered_words.append(word)
                
                rewritten = ' '.join(filtered_words)
        
        # Remove consecutive duplicate words (e.g., "particular particular")
        words = rewritten.split()
        filtered_words = []
        prev_word = None
        prev_count = 0
        for word in words:
            word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
            if word_lower != prev_word:
                filtered_words.append(word)
                prev_word = word_lower
                prev_count = 1
            elif word_lower == prev_word:
                prev_count += 1
                # Allow max 1 repeat for short words, 0 for long words
                if len(word_lower) <= 2 and prev_count <= 2:
                    filtered_words.append(word)
                # Block all repeats for longer words
        
        rewritten = ' '.join(filtered_words)
        
        # Fix common wrong synonyms and broken phrases (BEFORE removing patterns)
        # Fix wrong synonyms
        rewritten = re.sub(r'\bsensibility\b', 'sensitivity', rewritten, flags=re.IGNORECASE)  # sensibility ‚Üí sensitivity
        
        # Fix broken phrases
        rewritten = re.sub(r'\bbasic and care\b', 'primary and community care', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bin basic and care\b', 'in primary and community care', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bas well as settings\b', 'and community settings', rewritten, flags=re.IGNORECASE)
        
        # Restore context when "environments" appears in medical/healthcare context
        # Look for patterns like "in environments" or "environments where" in healthcare context
        if 'cognitive' in rewritten.lower() or 'health' in rewritten.lower() or 'care' in rewritten.lower():
            rewritten = re.sub(r'\benvironments\b(?=\s+(?:where|in|for|that|with))', 'community settings', rewritten, flags=re.IGNORECASE)
        
        # Remove common model collapse patterns (more aggressive)
        collapse_patterns = [
            r'\bparticular\s+particular\s+particular\b',
            r'\bcharacteristic\s+characteristic\s+characteristic\b',
            r'\bassociated\s+associated\s+associated\b',
            r'\bplus\s+plus\s+plus\b',
            r'\bparticular\s+characteristic\s+associated\b',
            r'\bplus\s+[^\s]+\s+plus\b',  # "plus X plus" pattern
            r'\bfront\s+front\s+front\b',
            r'\bfriends\s+friends\s+friends\b',
            r'\btroublesome\w*\s+troublesome\w*\b',  # Troublesome variations
        ]
        for pattern in collapse_patterns:
            rewritten = re.sub(pattern, '', rewritten, flags=re.IGNORECASE)
        
        # Remove standalone "plus" and "Plus" (often artifacts)
        rewritten = re.sub(r'\bplus\b', '', rewritten, flags=re.IGNORECASE)
        rewritten = re.sub(r'\bPlus\b', '', rewritten)
        
        # Remove words ending with "plus" (e.g., "treatmentplus", "lovedlinessesplus")
        rewritten = re.sub(r'\b\w+plus\b', '', rewritten, flags=re.IGNORECASE)
        
        # Remove made-up words (long concatenated words like "lovedlinesses", "organismalITY")
        # Words longer than 15 characters that aren't proper nouns
        words = rewritten.split()
        filtered_words = []
        for word in words:
            word_clean = word.strip('.,!?;:()[]{}"\'-')
            # Keep words that are reasonable length or are proper nouns (start with capital)
            if len(word_clean) <= 15 or (word_clean[0].isupper() and len(word_clean) <= 20):
                filtered_words.append(word)
        rewritten = ' '.join(filtered_words)
        
        # Remove words with excessive repeated characters (e.g., "plusplusplus", "lifestyleplus")
        rewritten = re.sub(r'\b\w*([a-z])\1{3,}\w*\b', '', rewritten, flags=re.IGNORECASE)
        
        # Final cleanup
        rewritten = re.sub(r'\s+', ' ', rewritten)  # Multiple spaces to single
        rewritten = rewritten.strip()
        
        # If output is too garbled or too short, return a fallback
        remaining_words = rewritten.split()
        if len(remaining_words) < 5:  # Too short
            rewritten = request.text  # Fallback to original
        
        # Check if output is mostly garbage (contains too many suspicious patterns)
        # Count suspicious patterns (not just words)
        suspicious_patterns = [
            'troublesome', 'troubleshooting', 'particular', 'characteristic',
            'associated', 'togetherness', 'built', 'frontline', 'friends',
            'fixtures', 'outfitwear', 'accessory', 'framework', 'array'
        ]
        suspicious_count = sum(1 for w in remaining_words 
                             if any(pattern in w.lower() for pattern in suspicious_patterns))
        
        if len(remaining_words) > 0:
            suspicious_ratio = suspicious_count / len(remaining_words)
            
            if suspicious_ratio > 0.2:  # More than 20% contain suspicious patterns
                # Try to salvage by keeping only clean words
                filtered = [w for w in remaining_words 
                          if not any(pattern in w.lower() for pattern in suspicious_patterns)]
                if len(filtered) >= 10:  # Keep if we have at least 10 good words
                    rewritten = ' '.join(filtered)
                # If still too garbled, try to extract meaningful sentences
                elif len(filtered) >= 5:
                    # Keep first meaningful sentence
                    sentences = ' '.join(filtered).split('.')
                    if sentences and sentences[0].strip():
                        rewritten = sentences[0].strip() + '.'
                    else:
                        rewritten = ' '.join(filtered)
                # Otherwise, return original text as fallback
                else:
                    rewritten = request.text  # Fallback to original if too garbled
        
        # If output is too short or identical to input, try with slightly adjusted parameters
        if len(rewritten.split()) < 3 or rewritten.lower().strip() == request.text.lower().strip():
            # Retry with slightly more variation but still controlled
            retry_temperature = min(0.8, temperature + 0.2)  # Cap at 0.8
            retry_repetition_penalty = min(1.8, repetition_penalty + 0.2)  # Cap at 1.8
            retry_length_penalty = min(1.3, length_penalty + 0.1)  # Cap at 1.3
            
            retry_gen_kwargs = {
                "max_length": max_output_length,
                "min_length": min_output_length,
                "num_beams": num_beams + 1,
                "early_stopping": True,
                "repetition_penalty": retry_repetition_penalty,
                "length_penalty": retry_length_penalty,
                "no_repeat_ngram_size": 3,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
            }
            
            # Use sampling for retry
            retry_gen_kwargs.update({
                "do_sample": True,
                "temperature": retry_temperature,
                "top_p": 0.9,
                "top_k": 50,
            })
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    **retry_gen_kwargs
                )
            rewritten = tokenizer.decode(
                outputs[0], 
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            # Remove prompt prefixes
            if rewritten.startswith("humanize:"):
                rewritten = rewritten.replace("humanize:", "").strip()
            elif rewritten.startswith("paraphrase:"):
                rewritten = rewritten.replace("paraphrase:", "").strip()
            
            # Fix encoding issues
            rewritten = re.sub(r'√¢√¢√¢[^\s]*', '', rewritten)  # Remove mojibake patterns
            rewritten = re.sub(r'[^\x20-\x7E\u00A0-\uFFFF]+', '', rewritten)  # Keep only valid Unicode
            
            # Apply same aggressive post-processing as above
            BLACKLIST_WORDS = {
                'particular', 'characteristic', 'associated', 'plus', 'front', 'friends',
                'troublesome', 'troubleshooting', 'troublesomeness', 'troubleworthyness',
                'otherworldlinestrezziness', 'troubleshoving', 'quizziness', 'heckplus',
                'hellabrinism', 'homeplus', 'friendswear', 'lovedplus', 'friendslines',
                'builtpertaining', 'associateddication', 'associatedwith', 'togetherness',
                'tenduousness', 'senselessness', 'latterness', 'firmness', 'array',
                'fixtures', 'outfitwear', 'accessory', 'pertaining', 'framework'
            }
            
            words = rewritten.split()
            word_counts = {}
            for word in words:
                word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
                if len(word_lower) > 3:
                    word_counts[word_lower] = word_counts.get(word_lower, 0) + 1
            
            total_meaningful_words = sum(1 for w in words if len(w.lower().strip('.,!?;:()[]{}"\'-')) > 3)
            if total_meaningful_words > 0:
                threshold = max(2, int(total_meaningful_words * 0.15))
                suspicious_words = {w for w, count in word_counts.items() if count > threshold}
                
                for word_lower, count in word_counts.items():
                    if word_lower in BLACKLIST_WORDS and count > 1:
                        suspicious_words.add(word_lower)
                
                if suspicious_words:
                    filtered_words = []
                    seen_suspicious = {}
                    for word in words:
                        word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
                        
                        if word_lower in BLACKLIST_WORDS:
                            if word_lower not in seen_suspicious:
                                seen_suspicious[word_lower] = 0
                            seen_suspicious[word_lower] += 1
                            if seen_suspicious[word_lower] == 1:
                                filtered_words.append(word)
                            continue
                        
                        if word_lower not in suspicious_words or len(word_lower) <= 3:
                            filtered_words.append(word)
                        else:
                            if word_lower not in seen_suspicious:
                                seen_suspicious[word_lower] = 0
                            seen_suspicious[word_lower] += 1
                            if seen_suspicious[word_lower] == 1:
                                filtered_words.append(word)
                    rewritten = ' '.join(filtered_words)
            
            # Remove consecutive duplicates
            words = rewritten.split()
            filtered_words = []
            prev_word = None
            prev_count = 0
            for word in words:
                word_lower = word.lower().strip('.,!?;:()[]{}"\'-')
                if word_lower != prev_word:
                    filtered_words.append(word)
                    prev_word = word_lower
                    prev_count = 1
                elif word_lower == prev_word:
                    prev_count += 1
                    if len(word_lower) <= 2 and prev_count <= 2:
                        filtered_words.append(word)
            
            rewritten = ' '.join(filtered_words)
            
            # Remove collapse patterns
            collapse_patterns = [
                r'\bparticular\s+particular\s+particular\b',
                r'\bcharacteristic\s+characteristic\s+characteristic\b',
                r'\bassociated\s+associated\s+associated\b',
                r'\bplus\s+plus\s+plus\b',
                r'\bplus\s+[^\s]+\s+plus\b',
                r'\bfront\s+front\s+front\b',
                r'\bfriends\s+friends\s+friends\b',
            ]
            for pattern in collapse_patterns:
                rewritten = re.sub(pattern, '', rewritten, flags=re.IGNORECASE)
            
            rewritten = re.sub(r'\bplus\b', '', rewritten, flags=re.IGNORECASE)
            rewritten = re.sub(r'\s+', ' ', rewritten).strip()
            
            # Fallback check
            remaining_words = rewritten.split()
            if len(remaining_words) > 0:
                suspicious_count = sum(1 for w in remaining_words 
                                     if w.lower().strip('.,!?;:()[]{}"\'-') in BLACKLIST_WORDS)
                if suspicious_count > len(remaining_words) * 0.3:
                    filtered = [w for w in remaining_words 
                              if w.lower().strip('.,!?;:()[]{}"\'-') not in BLACKLIST_WORDS]
                    if len(filtered) >= 5:
                        rewritten = ' '.join(filtered)
                    elif len(rewritten.split()) < 10:
                        rewritten = request.text
        
        processing_time = time.time() - start_time
        
        return RewriteResponse(
            original=request.text,
            rewritten=rewritten,
            tone=request.tone,
            strength=request.strength,
            processing_time=processing_time
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error generating paraphrase: {str(e)}")


@app.get("/api/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": str(device) if device else None
    }


@app.post("/api/reload-model")
async def reload_model():
    """Reload the model (useful after training)"""
    global model, tokenizer, device
    try:
        # Clear existing model from memory
        if model is not None:
            del model
        if tokenizer is not None:
            del tokenizer
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Reload model
        load_model()
        return {
            "status": "success",
            "message": "Model reloaded successfully",
            "model_loaded": model is not None,
            "device": str(device) if device else None
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Failed to reload model: {str(e)}",
            "model_loaded": model is not None
        }


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=config['api']['host'],
        port=config['api']['port'],
        reload=True
    )
